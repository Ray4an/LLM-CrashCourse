# LLM-CrashCourse

Here's a 9-hour crash course curriculum for learning to build LLM applications.

All resources are free.

ğ—£ğ—”ğ—¥ğ—§ ğŸ­ - ğ—” ğ—šğ—˜ğ—¡ğ—§ğ—Ÿğ—˜ ğ—œğ—¡ğ—§ğ—¥ğ—¢ ğ—§ğ—¢ ğ—Ÿğ—Ÿğ— ğ—¦ (1h 30m)

Intro & overview of language models, next-word prediction, embeddings, cosine similarity, semantic search

Resources: <br>
ğŸ“º 30m: Simple intro (MBerm) https://lnkd.in/eCfYMdUK <br>
ğŸ“– 40m: llms nohype (MRied) https://lnkd.in/eY6Pd9bJ <br>
ğŸ“º 10m: Next wrd prd (LBouc) https://lnkd.in/ergd9FT2 <br>
ğŸ“º 10m: Sem search (LSerr) https://lnkd.in/eRnnhmuj <br>

---

ğ—£ğ—”ğ—¥ğ—§ ğŸ® - ğ—§ğ—¥ğ—”ğ—¡ğ—¦ğ—™ğ—¢ğ—¥ğ— ğ—˜ğ—¥ ğ—”ğ—¥ğ—–ğ—›ğ—œğ—§ğ—˜ğ—–ğ—§ğ—¨ğ—¥ğ—˜ (1h 30m)

Encoder-decoder architecture, masking, attention, transformers, GPTs

Resources: <br>
ğŸ“º 30m: Visual llm (3Blu1Br) https://lnkd.in/egJPUhuX <br>
ğŸ“º 20m: Enc-Dec (JStarm) https://lnkd.in/eJNfAKZ8 <br>
ğŸ“º 10m: Attention (LSerr) https://lnkd.in/eEFrS5uV <br>
ğŸ“º 15m: Transformers (LSerr) https://lnkd.in/eVcnZxad <br>
ğŸ“º 15m: GPTs (DataBrks) https://lnkd.in/es9bKS-z <br>

---

ğ—£ğ—”ğ—¥ğ—§ ğŸ¯ - ğ—£ğ—¥ğ—¢ğ— ğ—£ğ—§ ğ—˜ğ—¡ğ—šğ—œğ—¡ğ—˜ğ—˜ğ—¥ğ—œğ—¡ğ—š (2h 0m)

Zero/one/few-shot, chain-of-thought, self-consistency, generated knowledge, prompt chaining, ReAct

Resources: <br>
ğŸ“º 60m: PE Overview (ESarav) https://lnkd.in/eFF2Yc7C <br>
ğŸ“– 90m: Prompt Eng (DAIR) https://lnkd.in/eaqPT63j <br>
ğŸ“– 30m: PE Guide (Brex) https://lnkd.in/etPjrkvw <br>
âš’ï¸ 0m: LangChain Hub https://lnkd.in/e_pt2cVx <br>

---

ğ—£ğ—”ğ—¥ğ—§ ğŸ° - ğ—–ğ—›ğ—”ğ—œğ—¡ğ—œğ—¡ğ—š, ğ—¥ğ—”ğ—š, ğ—©ğ—˜ğ—–ğ—§ğ—¢ğ—¥ ğ——ğ—•ğ—¦, ğ—”ğ—šğ—˜ğ—¡ğ—§ğ—¦ (2h 30m)

Langchain, RAG, vector databases, LlamaIndex, Open AI functions

Resources: <br>
ğŸ“º 30m: LngCh ckbk1 (GKam) https://lnkd.in/eRTUH3rZ <br>
ğŸ“º 30m: LngCh ckbk2 (GKam) https://lnkd.in/ehXZNsRg <br>
âš’ï¸ 0m: LngCh tutorials (GKam) https://lnkd.in/eSyFQwhm <br>
ğŸ“– 30m: Adv RAG (HugFace) https://lnkd.in/eSqe-UCD <br>
ğŸ“º 5m: Vector dbs (Fireshp) https://lnkd.in/et3Zz8-k <br>
ğŸ“º 10m: LC+Pinecone (GKam) https://lnkd.in/e5E6Fv46 <br>
ğŸ“º 20m: LlamaIdx (engprmpt) https://lnkd.in/eYRFG-wg <br>
ğŸ“º 30m: OpenAI fn/agnt (auto) https://lnkd.in/esZa7c3s <br>

---

ğ—£ğ—”ğ—¥ğ—§ ğŸ± - ğ—™ğ—œğ—¡ğ—˜ğ—§ğ—¨ğ—¡ğ—œğ—¡ğ—š (1h 30m)

Feature-based finetuning, LoRA, RLHF

Resources: <br>
ğŸ“– 15m: Finetuning (SRasc) https://lnkd.in/eTdFHQS5 <br>
ğŸ“– 15m: FT v PrmpEng (NBant) https://lnkd.in/ekeHKWPe <br>
ğŸ“º 30m: RLHF intro (HugFac) https://lnkd.in/etsH53ri <br>
ğŸ“– 15m: RLHF guid (Lblerr) https://lnkd.in/e57dsuEp <br>
ğŸ“– 15m: LoRA (HugFac) https://lnkd.in/ervJK9C7 <br>


---

# 10 Supervised fine-tuning (SFT) tips when starting a new LLM project. Here are our default settings for GPUs (A100 and newer):

3ï¸âƒ£ Start with 3 epochs <br>
ğŸ“‰ LR: 2e-5 with a cosine schedule & 0.1 warmup ratio <br>
ğŸ”— Apply Packing to combine samples up to a sequence length (2048) <br>
ğŸ“ Try Global BS of 256/512 (e.g., BS=16 per device, grad_acc=2/4 on 8xH100s) <br>
âš¡ Use Flash Attention v2 with bf16 & tf32 (to speed up remaining fp32 calculations) <br>
ğŸ’¾ Enable gradient checkpointing to save memory <br>
ğŸš€ Opt for â€œadamw_torch_fusedâ€ (10% speed up) or â€œadamw_torchâ€ optimizer <br>
ğŸ–¥ï¸ Deepspeed & FSDP both work well for distributed training <br>
âš™ï¸ Consider LoRA for quicker iterations with less compute <br>
ğŸ¤— Use the SFTTrainer from Hugging Face TRL <br>
